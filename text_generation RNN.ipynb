{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GUR3xx_x4U1q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "imdb_reviews, _ = tf.keras.datasets.imdb.load_data()"
      ],
      "metadata": {
        "id": "9zNtBVBT5IKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40a94a38-43f0-4b7e-acf4-f68bd73782b5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 10000\n",
        "imdb_reviews = imdb_reviews[:num_samples]"
      ],
      "metadata": {
        "id": "pjacXy4XBqhN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert reviews to strings\n",
        "imdb_reviews= [' '.join(map(str, review)) for review in imdb_reviews[0]]"
      ],
      "metadata": {
        "id": "PUl0Wyct8wJd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess dataset\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(imdb_reviews)\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "fHsNo-5X5Jpd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(reviews, tokenizer, vocab_size, max_seq_length, batch_size):\n",
        "    while True:\n",
        "        for i in range(0, len(reviews), batch_size):\n",
        "            batch_reviews = reviews[i:i+batch_size]\n",
        "            batch_reviews = [' '.join(map(str, review)) for review in batch_reviews]  # Convert to list of strings\n",
        "            sequences = tokenizer.texts_to_sequences(batch_reviews)\n",
        "            sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='pre')\n",
        "            X = sequences[:, :-1]\n",
        "            y = tf.keras.utils.to_categorical(sequences[:, -1], num_classes=vocab_size)\n",
        "            yield X, y"
      ],
      "metadata": {
        "id": "WKOJ3iRp5lPD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and validation sets\n",
        "X_train, X_val = train_test_split(imdb_reviews, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "PyiDO4PFSbUJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "max_seq_length = max(len(review.split()) for review in imdb_reviews)\n",
        "embedding_dim = 256\n",
        "lstm_units = 128\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "OISg6iTVTe2J"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_seq_length-1),\n",
        "    LSTM(units=lstm_units),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "TNGy5MEy6WXT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "xyA4Dt9Z6q5_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model using data generator\n",
        "train_generator = data_generator(X_train, tokenizer,vocab_size, max_seq_length, batch_size)\n",
        "steps_per_epoch = len(X_train) // batch_size\n",
        "validation_steps = len(X_val) // batch_size"
      ],
      "metadata": {
        "id": "zOLOqMrOIBwU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=10, validation_data=train_generator, validation_steps=validation_steps)"
      ],
      "metadata": {
        "id": "xc8HOy3h6v74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0016e938-1997-4794-fda2-ca0ce35694ee"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 73s 111ms/step - loss: 2.4624 - val_loss: 2.0855\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 68s 109ms/step - loss: 2.0592 - val_loss: 2.0576\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 70s 112ms/step - loss: 2.0593 - val_loss: 2.0516\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 73s 116ms/step - loss: 2.0502 - val_loss: 2.0018\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 68s 109ms/step - loss: 1.9753 - val_loss: 1.9737\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 69s 111ms/step - loss: 1.9467 - val_loss: 1.9235\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 70s 112ms/step - loss: 1.9358 - val_loss: 1.9229\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 70s 111ms/step - loss: 1.9265 - val_loss: 1.9227\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 71s 113ms/step - loss: 1.9166 - val_loss: 1.9151\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 73s 117ms/step - loss: 1.9080 - val_loss: 1.8955\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7daf3cb9bfa0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "seed_text = \"The sun\"\n",
        "for _ in range(100):\n",
        "    encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    encoded = pad_sequences([encoded], maxlen=max_seq_length-1, truncating='pre')\n",
        "    y_pred = model.predict(encoded)\n",
        "    next_word = tokenizer.index_word[y_pred[0]]\n",
        "    seed_text += \" \" + next_word\n",
        "print(seed_text)"
      ],
      "metadata": {
        "id": "IjAWr3S26xi_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "568cb9d9-bccb-49cb-c048-e6b48d97a763"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 496ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'numpy.ndarray'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-8be981b121b9>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pre'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mnext_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mseed_text\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnext_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ]
    }
  ]
}